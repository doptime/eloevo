# 一种多巴胺驱动的欢乐学习算法

## 系统设计原则
    1. 最大化用户的信息增益。
       1. 这个系统的设计是模拟婴儿早期对知识的累积和学习方式。
       2. 用户尽可能在学习过程中，采用间接应用知识点的方式，而不是直接学习知识点的方式学习。也就是把知识的建构交给后台神经系统来完成建构。
       3. 用户并不追求绝对的精确认知和正确性。而是追求用户尽可能少痛感地在知识路径中游乐。

## 游戏设置
总共N道题，对应N个确切的学习小主题，(Problem_i。∈[1,N]),N Problem Topics,Problem_i。
N道题依序构成学习路径；N道题之间按语义相近度，互为邻域问题。

每个主题题目具有：
    知识点：
        - ​每个主题包含4组知识点(j∈[1,4])。
        - 可以拆包成不同权重的四份知识点。ProblemAnswer_i,j ProblemAnswerWeightt_i,j 
        - 知识点的权重是整数 1到10之间的值。
    每个知识点有一个权重值(ProblemAnswerWeight_i,j)，表示该知识点在该主题中的重要性。

## 游戏规则：
    0.把先前学习材料中基于FSRS (Free Spaced Repetition Scheduler)  算法筛选出的今日学习内容合并到当前学习内容中。作为总体学习主题包。    
    1.先使用LLM，基于间隔重复算法和内容最优化调度，选出1个用来学习主题和k个邻域主题(用来提供内容混淆)（k∈[1,3]），形成一个学习主题组。
    2.把学习主题组的每一个主题的4个选项：分别随机放入4个选项卡当中。对选项卡的选项顺序进行随机打乱。到此，确保每个选项卡都包含学习主题组中每主题的一个知识点。
    4.对主题进行提问，比如给定 什么是词语认真
    5.对全部4个选项卡，要求用户点击中的一个条目。
    7.统计回答情况,更新用户模型相关的参数。
    8.给与游戏激励
​    8.重复1-7，直到所有主题都完成。
    9.每个主题完成后，在Hall-Of-Fame中显式排名信息。
       总积分榜单包括总权重得分和。排行榜单有季度榜单和年度榜单。
## 用户模型更新:
    1.给定问题，统计选项回答时长，统计回答的得分。转化为用户的分位数时长和分位数得分。
    2.使用分位数时长和分位数得分 计算用户评价Grade；
        Grade (g) 是用户对卡片回忆难度的主观评分（1=Again, 2=Hard, 3=Good, 4=Easy）。correctness∈{0,0,25,0.5,0.75,1},correctness 定义为4个选项卡选中正确条目的数量占比; timePercentile∈[0,1.0], timePercentile the lower the better。
        - 采用公式g= 4*((α*(correctness) + β*timePercentile)); g= max(min(g, 4.0), 1.0),g是连续的浮点数; 
        - 采用公式g= 4*sqrt( (α*(correctness^2) + β*(1-timePercentile)^2) ); g= max(min(g, 4.0), 1.0),g是连续的浮点数; 
          为什么？平方根和平方项强调高正确性和低 timePercentile（因为越低越好），同时平滑微小差异的影响。这使得配方对缓慢但正确的反应更宽容，与婴儿般的探索相一致。
        - $g = 4 \times \sqrt{\alpha \times \text{correctness}^2 + \beta \times (1 - \text{timePercentile})^2}, \quad g = \max(\min(g, 4.0), 1.0)$
        - α, β:= 0.5 ,0.5, α 和 β是随用户体验动态调整的。它根据用户的已回答问题总数 （N_user） 调整 α 和 β：
            对于新用户（N_user < 50 人）：α = 0.7，β = 0.3（优先考虑正确性以鼓励探索）。
            对于中级用户 （50 ≤ N_user < 200）：α = 0.6，β = 0.4。
            对于有经验的用户 （N_user ≥ 200）：α = 0.5，β = 0.5（随着速度变得更加相关，权重相等）。
            为什么？婴儿在早期优先考虑理解而不是速度，但随着他们获得经验，探索效率变得更加重要。
            这种动态调整反映了这一进展。
            多巴胺益处：根据用户体验定制权重可确保奖励符合预期，从而在整个学习阶段保持积极的 RPE。
        - 其中:
          - timePercentile 使用几何平均值将（特定于问题的百分位数）和（特定于用户的百分位数）组合在一起，并按问题和用户响应计数加权。 平滑因子 （k） 增加了容错性，防止对新问题或用户进行严厉的处罚。
          - timePercentile = (percentile_problem ^ w_problem) * (percentile_user ^ w_user)
          - percentile_problem 是对该题目的回答时长分布的分位数，percentile_user 是对用户所有题目的回答时长分布的分位数。
          - w_problem = _w_problem / (_w_problem + _w_user),w_user = _w_user / (_w_problem + _w_user)
          - _w_problem = sqrt(N_problem + k), _w_user = sqrt(N_user + k);
          - N_problem，是回答过该题目的总用户数; N_user，是该用户回答过的总题目数; 
          - 平滑因子 或  伪计数 k=4 或k=10;
        - 
         
    1. 并更新FSRS卡片的难度系数 D和稳定性 S。使用go-fsrs 实现
## 游戏激励:
    1.可变奖励（Variable Rewards）:
        引入“大暴击”概念。
            首次点击正确且包含最高权重的知识点，额外随机奖励 1.9-2.1 倍权重积分，除了得分外，可以触发一个“完美命中！”的动画。如果点击错误，则不得分。
        引入“小暴击”概念。
            第二次点击正确且包含最高或次高权重的知识点，额外随机奖励 0.9-1.1 倍权重积分。除了得分外，可以触发一个“非常重要！”的动画。如果点击错误，则不得分。
        引入“超级暴击”概念。用户连续3次以上完美命中，完美命中升级成为超级暴击，触发一个“超级暴击！”的动画，额外随给予3倍-3.5倍的权重积分。
    1. “超级彩蛋”的稀缺性和随机性: 超级彩蛋的获取条件可以变得更“神秘”。累积2个超级暴击，则有0.5的概率“嘭”地跳出一个巨大的金彩蛋，并出现在奖励区。这种“意外之喜”会带来巨大的多巴胺飙升，让用户对下一次的“大奖”充满期待。
       超级彩蛋会在结束的时候自动爆炸，每个彩蛋为总分增加5%-15%的随机奖励。
       超级彩蛋爆炸后，一些兔子会从彩蛋中跳出来，跑走消失，彩花满屏落下，幕布上滚动最后增加的分数。同时总分相应增加。然后幕布上滚，跳出Hall-Of-Fame的排名信息。
    2. 增加“差一点就成功”（Near Miss）的反馈: 如果用户的点击的选项没有命中目标选项，比如：“哇！就差一点点金彩蛋就是我的了！” “哇！就差一点点超级暴击！” “哇！就差一点命中！” “哇哦！我错过了！” 这会利用“损失厌恶”（Loss Aversion）和“蔡格尼克记忆效应”（Zeigarnik Effect，对未完成任务的记忆更深刻），极大地激发用户再来一次的动力。 第一人称视角用婴儿的心理活动和小宝宝的声音来表达，这样可以让用户更有代入感和接纳感。
       

## 动态难度调节（Dynamic Difficulty Adjustment）:
    对于一个全新的主题，可以先从“1主+1邻”开始，即8个知识点重组成4组。让用户先建立初步的认知。若连续3题正确率 > 80%，k 增加1，最高为3；若正确率 < 50%，k 减少1，最低为1

## 一些关键思路和细节：
​   设计思路 多巴胺是一种大脑学习的专注性算法。驱动奖励预测误差变小。从而使得学习者保持专注。
   - 正向RPE (Reward Prediction Error, RPE)：当实际奖励 超过 预期奖励时，多巴胺神经元会强烈放电。这会强化导致该行为的神经通路，让你更想重复这个行为。“暴击”和“彩蛋”就是完美利用了这一点。
   - 负向RPE(Reward Prediction Error, RPE)：当实际奖励 低于 预期奖励时，多巴胺放电会受到抑制。这会让你避免重复导致失败的行为，并促使你调整策略。“差一点”反馈就是一种温和的负向RPE。

   - 设计思路 几乎任何点击都会有欢乐的奖励提示。
   - 设计思路 这种反思有第一人称心理活动的视角点评呈现。学习者的心理活动可以帮助学习者更好地理解和记忆。而第一人小宝宝称视角强化了心理活动的感觉。
   - 细节 第一人称视角点评加深了对细节的反复反思，
   - 领域主题和当前知识点的作为学习材料的反复出现，为高速认知上的高速处理提供认知预备。从而让整个学习过程有流畅和高效的爽感。
        熟练阶段: 如果用户答对，系统自动增加难度，变为“1主+2邻”，乃至最终的“1主+3邻”，反之减少一个领域问题，直到最少“1主+1邻”。

   - 问题路径:答题路径与邻域问题的编排：引入“间隔重复”与“遗忘曲线”，间隔重复算法采用golang 包 github.com/open-spaced-repetition/go-fsrs 实现


现在请从大脑学习的第一性原理，如多巴胺工作机制和其它认知心理学原理，改善这个学习流程。特别是改进评级Grade 的计算方式